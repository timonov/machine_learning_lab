# Описание полученных результатов
#### График зависимости точности предсказания алгоритма от количества тренировочных векторов _(dependence method's accuracy of the value training sample.png)_.
Выводы:
1. _GradientBoostingClassifier_ и _RandomForestClassifier_ показывают более точные результаты распознавания, чем _NaiveBayes_
2. Появилась возможность выделить оптимальный для всех трех методов размер обучающей выборки - 90 (векторов).
3. Наилучшая точность _NaiveBayes_ для данной выборки является страртовым значением точности для _RandomForestClassifier_ алгоритма.
4. _GradientBoostingClassifier_ показал себя как меннее стабильный алгоритм среди трех представленных (много пиков).

#### Матрицы неточностей в предсказании _(confusion_matrix_bnb-new.png, confusion_matrix_frc-new.png, confusion_matrix_gbc-new.png)_
Выводы:
1. С размеров выборки, равным 90, предсказания получились намного более лучше, нчем в случае использования рандомного значения 150 (см. _confusion_matrix_bnb-old.png_ и другие)
2. Все три представленных алгоритма с максимальной точностью распознали первый класс данных в выборке ***(!!! следует уточнить в описании выборки, чему соответствует первый класс и насколько велико представителей первого класса в выборке)***. Возможно такой хороший результат зависит от этого фактора.
3. _GradientBoostingClassifier_ уверенно распознал №0 и №2 классы, с вероятностью 1/2 класс №5. Остальные же классы детектировать не удалось.
4. _NaiveBayes_ с вероятностью 0,55 верно определил классы №0, №5 и №2, однако, при этом посчитал, что 2 класс является так же и 5м классом (p=0,45). Остальные классы выборки определить метод верно детектировать не мог.
5. Почти такие же результаты, как и у _NaiveBayes_,  у _RandomForestClassifier_ метода: с вероятностью 0,5 были верно определены №0 и №2 классы, второй класс так же в редких случаях детектировался как 5-й. Классы начиная с 3-го классифицировались как 2-ой класс.
